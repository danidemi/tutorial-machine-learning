{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli alberi decisionali sono algoritmi di ML molto versatili che possono essere utilizzati sia per problemi di regressione che di classificazione."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fare training e visualizzare un albero decisionale\n",
    "\n",
    "Iniziamo importando il dataset degli iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 righe e 4 colonne\n",
    "\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Per comodità utlizziamo solamente petal length e petal width\n",
    "\n",
    "X = iris.data[: , 2:]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth = 2)\n",
    "tree_clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Come si rappresenta?\n",
    " Abbiamo due modi per visualizzare un albero decisionale: \n",
    " 1. usare tree.plot_tree(tree_clf)\n",
    " 2. oppure usare la libreria export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "tree.plot_tree(tree_clf, \n",
    "               class_names=iris.target_names, \n",
    "               feature_names = iris.feature_names[2:], \n",
    "               filled=True, \n",
    "               rounded = True\n",
    "              )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install graphviz\n",
    "# https://graphviz.org/download/\n",
    "from sklearn.tree import export_graphviz \n",
    "import os\n",
    "import graphviz\n",
    "\n",
    "dot_data = export_graphviz(tree_clf, out_file=None, \n",
    "                      feature_names=iris.feature_names[2:],  \n",
    "                      class_names=iris.target_names,  \n",
    "                      filled=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leggere un albero decisionale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supponiamo di avere un iris e volerlo classificare. Per prima cosa si parte dal root node (depth = 0). Questo nodo ci chiede se la lunghezza del petalo è <= 2.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se la risposta è sì, allora dobbiamo muoverci a sinistra e arriviamo ad una foglia (un nodo senza figli). Il fiore viene quindi classificato come un iris di tipo setosa. Se la risposta è invece no, ci si sposta sul nodo di destra: a questo punto bisogna invece guardale la larghezza del petalo, se questa è minore o uguale a 1.75 si finisce nella foglia verde, e quindi l'iris viene classificato come versicolor; se la larghezza è maggiore di 1.75 ci si muove a destra e l'iris viene classificato come virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DOMANDA:\n",
    "come viene classificato un iris che ha lunghezza dei petali pari a 2.5 cm e larghezza pari a 3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attributo samples, presente in ogni nodo, rappresenta il numero di istanze del dataset che si applicano ad un determinato nodo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VALUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attributo value è rappresentato da una lista che ha tanti elementi quante sono le classi: nel nostro caso 3.\n",
    "I valori corrispondono al numero di fiori del dataset che \"arrivano\" a quel nodo. Se consideriamo ad esempio la foglia verde abbiamo value = [0, 49, 5], questo significa che il nodo si applica a 54 fiori (49 + 5), 49 di tipo versicolor e 5 di tipo  virginica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GINI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'attributo gini è una misura dell'impurità del nodo: tanto più l'indice è alto tanto più il nodo è impuro. Un nodo è considerato come 'puro' se gini = 0 cioè se tutte le istanze che si applicano a quel nodo appartengono alla stessa classe.Un esempio di nodo puro è dato dalla foglia arancione:\n",
    "questo nodo si applica a 50 fiori che sono tutti della classe setosa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideriamo il nodo verde: per calcolare l'indice di gini si procede come segue:\n",
    "    \n",
    "    1 - (0/54)^2 - (49/54)^2 - (5/54)^2 = 0,168 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "plot_step = 0.2\n",
    "\n",
    "# Griglia di punti\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "Z = tree_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "#plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30)\n",
    "plt.xlabel(iris.feature_names[2])\n",
    "plt.ylabel(iris.feature_names[3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La zona rossa è quella definita dalla foglia arancione (petal_length <= 2.45)\n",
    "La zona blu dalla condizione petal_length > 2.45 e petal width > 1.75\n",
    "La zona gialla dalla condizione petal_length > 2.45 e petal width <= 1.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per fare il training del modello scikit-learn utilizza un'algoritmo chiamato CART (Classification and Regression Tree) che produce solamente alberi binari: ogni nodo ha al massimo due foglie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'idea che sta alla base di questo algoritmo è quella di trovare una coppia (valore soglia, feature), nel nostro caso (2.45, petal length), che dia la maggior purezza possibile (non è altro che una media pesata)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questo procedimento va a avanti finchè l'albero non arriva alla massima profondità oppure non ha più modo per massimizzare la purezza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ESERCIZIO: utilizzando il dataset iris fai il training del modello utilizzando tutte le features a disposizione e disegna l'albero decisionale scegliendo come max_depth = 3 [soluzione](./soluzione/decisionTree_sol.ipynb).\n",
    "\n",
    "In base a come funziona il CART Algorithm, sai dire perchè l'albero ha esattamente quella struttura?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERFITTING VS UNDERFITTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[overfitting-underfitting](./9_overfitting_and_underfitting.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A differenza di un modello di regressione linerare, dove il numero di parametri è fissato a priori, gli alberi decisionali rappresentano invece modelli non parametrici, nel senso che il numero dei parametri del modello non è fissato a priori. Se non controllati questi modelli possono portare ad un problema di overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno degli iperparametri che ci permette di regolarizzare il modello è il max_depth: un max_depth troppo piccolo porterà ad avere un problema di underfitting mentre un numero troppo alto porterà ad avere un problema di overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ESERCIZIO: \n",
    "utilizzando il seguente dataset make_moons(n_samples=100, shuffle=True, noise = 0.2, random_state=6) \n",
    "1. fai il training di un albero decisionale senza definire la profondità massima;\n",
    "2. disegna l'albero;\n",
    "3. disegna il grafico dividendo in aree;\n",
    "4. leggendo la documentazione (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    " trova il modo di stampare quanto è profondo l'albero e il numero di foglie;\n",
    "5. quale caratteristica è comune a tutte le foglie?\n",
    "\n",
    " [soluzione](./soluzione/MOONS.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Come abbiamo già accennato all'inizio, gli alberi decisionali sono in grado di risolvere anche problemi di regressione.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X,y = make_regression(n_samples=200, n_features=1, bias=1, noise=6, shuffle=True)\n",
    "y = y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0],y, s=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=3)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz \n",
    "import os\n",
    "import graphviz\n",
    "\n",
    "dot_data = export_graphviz(tree_reg, out_file=None, \n",
    "                      feature_names=['X'],  \n",
    "                      class_names=['y'],  \n",
    "                      filled=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_min, x_max = X.min() - 1, X.max() + 1\n",
    "\n",
    "plot_step = 0.2\n",
    "x = np.arange(x_min, x_max, plot_step).reshape(-1, 1)\n",
    "Z = tree_reg.predict(x)\n",
    "plt.plot(x[:,0], Z)\n",
    "plt.scatter(X, y, s=30)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg.predict([[-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg.predict([[-3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg.predict([[-5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
